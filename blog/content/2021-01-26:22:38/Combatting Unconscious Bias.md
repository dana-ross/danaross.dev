The business world is waking up to the fact we can't simply blame poor diversity statistics on a *pipeline problem*. The systems we create to filter candidates are just as much to blame—if not more—than the systems that prepare candidates for the workforce. Whether you work in HR, you're trying to fill a role that will report to you, or you're a peer taking part in a panel interview, you owe it to every candidate to give them a fair shot.

It shouldn't be surprising when research shows discrimination against candidates based on ethnicity indicators such as their name, educational background, or past work experience. And, while companies say they want employees to *bring their whole self* to work, the same research shows no difference in responses to resumes from businesses that claim to value diversity! Candidates attempt to circumvent this by *whitening* their resumes, Anglicizing their names and backgrounds, which turns out to actually work in their favor:

> In fact, companies are more than twice as likely to call minority applicants for interviews if they submit whitened resumes than candidates who reveal their race—and this discriminatory practice is just as strong for businesses that claim to value diversity as those that don’t. <cite>[Minorities Who 'Whiten' Job Resumes Get More Interview](https://hbswk.hbs.edu/item/minorities-who-whiten-job-resumes-get-more-interviews)</cite>

## You can't automate good behavior

We thought we could automate our way out of these problems.

*Resume anonymization* or *blind hiring* has been a common approach. Anonymized resumes are ones stripped of elements such as candidates' names or the schools, in the hopes that what remains is objective criteria. This has turned out disasterous, with companies that value diversity overlooking minority candidates they would normally pursue with extra effort:

> When resumes were made anonymous, participating firms became less likely to interview and hire minority candidates. The gap in interview rates between non-minority and minority candidates widened by 10.7 percentage points, from 2.4 percentage points in the standard procedure to 13 percentage points in the anonymized procedure. At the hiring stage, the gap widened by 3.7 percentage points. <cite>[Unintended effects of anonymous resumes](https://www.povertyactionlab.org/case-study/unintended-effects-anonymous-resumes)</cite>

There's also the problem of how one completely anonymizes a resume. You can easily remove a candidate's name, pictures, addresses, and other basic data. Other signals are harder to scrub:

* What school did the candidate attend? Which campus? Was it a religious institution?
* Did the candidate link to their GitHub or social media profile?
* Does the candidate's work history point to them living in a particular place? Do they have experiences with technologies that betray their age such as typewriter repair or COBOL?
* Did the resume include volunteer work or hobbies which indicate a particular background, race, or religious affiliation?

*Panel interviews* are another way to combat bias in hiring. Candidates speak with a diverse panel of current team members to not only get a fuller picture of their competencies, but to evaluate how they interact with people from different backgrounds. These can be an effective tool for combatting hiring bias if the panel interviewers are prepared to confront their own biases. But it's rare that panel interviewers are trained to avoid their own unconscious biases during training.

Amazon had to scrap an AI tool that unemotionally evaluated resumes when they realized [it preferred male candidates to female ones](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G): 

> But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.
> That is because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry.<cite>[Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)</cite>

## You must identify and face your biases

We all have biases. Even the most well intentioned and pure hearted of us may still suffer from [affinity bias](https://diversityjournal.com/13763-affinity-bias-conundrum-illusion-inclusion-part-iii/), the idea we react most positively to people who look like us, sound like us, and have similar backgrounds to ours. I'm not saying this to shame anyone, but to call out how similar we all really are. We're all human.

The first step to improving your own hiring practices is to admit you have biases, even if you're not able to name them yet.

It's vital you identify your biases and gain a sense of control over them. Otherwise you risk not only displaying bias in your hiring practices but in the rest of your life as well. Seek out a therapist who can work through these unconscious patterns with you, and who can give you tools to silence them.

## Practice *Controlled Processing*

Our minds process information through two strategies: [Automatic Processing and Controlled Processing](https://en.wikipedia.org/wiki/Automatic_and_controlled_processes). Automatic processing happens quicky, often without us even realizing it: You come to a door, you reach out and open it. You don't think through the process of extending your hand, grasping the knob, turning the knob, and pulling the door towards you.

Controlled processing happens slowly and deliberately. Think following a recipe, choosing an outfit in the morning, or planning a vacation.

We're usually overwhelmed when we review resumes. And we probably wouldn't be hiring if we weren't already swamped with work! It's tempting to *go with our gut* when making hiring decisions—in fact some people recommend it. But that's the wrong approach if our goal is a fair hiring process and a diverse team. That kind of quick decision making encourages Automatic Processing, opening the door to all sorts of reckless choices.

Consider how you can force yourself to use Controlled Processing instead. The [SPACE2 model](https://cultureplusconsulting.com/2018/10/17/six-proven-strategies-for-managing-unconscious-bias/) is a set of strategies that enable Controlled Processing:
> * *Slowing Down* — being mindful and considered in your responses to others
> * *Perspective Taking* — actively imagining the thoughts and feelings of others
> * *Asking Yourself* — active self-questioning to challenge your assumptions
> * *Cultural Intelligence* — interpreting a person’s behaviour through their cultural lens rather than your own
> * *Exemplars* — identifying counter-stereotypical individuals
> * *Expand* — the formation of diverse friendships

In my past hiring experience, I've deliberately slowed down to protect against biases I know are there from my upbringing. For example, I:

* Set a timer to make sure I spend no less than 5 minutes reading a resume, cover letter, and any supplementary links or documents
* Turn off email, Slack, and other distractions. A little soft music is ok when reading resumes or notes, but nothing that will keep me from focusing 100% on the candidate in front of me
* Create a spreadsheet where I need to articulate a *reason* why I'm rejecting candidates, and I forbid myself from copying & pasting a previous answer. I make myself provide a specific answer tailored to each candidate: "Prior experience is all with irrelevant tech stacks" or "Candidate has only 2.5 years' experience"
* Wait a minimum of two hours after an interview to revisit my notes and fill out any HR surveys or paperwork so I'm emotionally detached from the experience
* Make sure I'm not only considering a candidate's work experience but the environments that fostered it. Sometimes this involves researching the companies on a resume as well as the candidates. For example, a web agency plays by different rules than an investment bank, including testing procedures, compliance requirements, and even technology choices. A candidate profile may claim in something they never have had the chance to employ professionally.

Does this make evaluating resumes or candidate interviews take longer? You bet! But I can be more confident in my ability to evaluate candidates fairly.

## You can do this

Fair hiring isn't just a legal obligation, it's a moral one. You owe it to your candidates, your team, and yourself to take the time to do it right. Learn to silence those judgemental voices, employ Controlled Processing, and hold yourself accountable. You may be surprised how quickly it pays off in the quality of people you bring on board. And I'm sure you'll be happier with yourself for having done it.
